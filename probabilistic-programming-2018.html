<!DOCTYPE html>
<html>
  <head>
    <title>Probabilistic Programming: What are all the random numbers for?</title>
    <meta charset="utf-8">
    <style>
      @import url(https://fonts.googleapis.com/css?family=Yanone+Kaffeesatz);
      @import url(https://fonts.googleapis.com/css?family=Droid+Serif:400,700,400italic);
      @import url(https://fonts.googleapis.com/css?family=Ubuntu+Mono:400,700,400italic);

      body { font-family: 'Droid Serif'; }
      h1, h2, h3 {
        font-family: 'Yanone Kaffeesatz';
        font-weight: normal;
      }
      .remark-slide-content h1, h2, h3 { font-size: 1.6em; }
      .remark-slide-content { font-size: 1.8em; }
      .remark-code, .remark-inline-code { font-family: 'Ubuntu Mono'; }
    </style>
  </head>
  <body>
    <textarea id="source">

class: center, middle

# What are all the random numbers for?
## Robert Dodier
## Probabilistic Programming meetup, Oct. 17, 2018
### This presentation:
### https://github.com/robert-dodier/probabilistic-programming-2018

---
# Introduction

 * Software for Bayesian inference often makes use of streams of random numbers

  * PyMC3, Stan, BUGS, etc.

 * What are all those numbers for? For what purpose are we generating random numbers?
 What problem are we solving by generating random numbers?

 * I'll talk about the motivation for all that random stuff, and talk a little about alternatives
 that don't involve random numbers.

---
# Motivation 1

 * We need random numbers to compute integrals

  * Summation = adding up discrete items, integral = adding up continuous items

 * Next question follows immediately, why are we computing integrals?

---
# Motivation 2

 * At a fundamental level, we need integrals because
 
 `\(P(A \; \mathrm{or} \; B) = P(A) + P(B) + P(A \; \mathrm{and} \; B)\)`

  * When A and B are mutually exclusive, `\(P(A \; \mathrm{and} \; B) = 0\)`, so `\(P(A \; \mathrm{or} \; B) = P(A) + P(B)\)`

---

 * Suppose `\(S\)` is the union of mutually exclusive `\(S_1, \ldots, S_n\)`

  * i.e. `\(x \in S = x \in S_1 \; \mathrm{or} \; \ldots \; \mathrm{or} \; x \in S_n\)`

  * Then `\(P(x \in S) = \sum_{k=1}^n P(x \in S_k)\)`

 * Suppose `\(S\)` is divided into infinitesimal bits, then `\(P(x \in S) = \int_S P(x) dx\)`

---
# Examples of integrals, in words

 * Find probability that some parameter falls in a specified set

  * e.g. parameter is positive, parameter is below a threshold, parameter is within certain distance of a point

---

 * Make a prediction about an interesting variable, averaging over less-interesting parameters

  * e.g. rainfall as a function of elevation and temperature from a multi-parameter model

  * e.g. mechanical failure as a function of observed sensor values from a model incorporating previous mechanical states

---

 * Compute expected cost of some action, averaging over unknown states of nature

  * Estimate operating cost of a car, averaging over wear and tear, accidents, future price of fuel

  * Estimate net benefit of planting millions of trees, with future climate and effect of trees unknown

---
# Motivation 2, summary

 * What all these examples have in common is the need to compute total amounts or averages over some variables

 * These totals or averages are represented as integrals

 * Let's rephrase the previous examples as formulas involving integrals

---
# Examples of integrals, as math formulas

 * Find probability that some parameter falls in a specified set

 `\(P(x \in S) = \int_S P(x) dx\)`

  * We saw this one a few minutes ago.

---

 * Make a prediction about an interesting variable, averaging over less-interesting parameters

 * Given `\(P(x, y)\)`,

  `\(P(y) = \int_{-\infty}^{\infty} P(x, y) dx\)`
 
 * Deriving `\(P(y)\)` from `\(P(x, y)\)` is called marginalizing

---

 * Compute expected cost of some action, averaging over unknown states of nature

 `\(E[f] = \int_{-\infty}^{\infty} P(x) f(x) dx\)`

  * `\(E[f]\)` is the expected value of `\(f(x)\)` with respect to `\(P(x)\)`

  * `\(f\)` is typically interpreted as a cost or benefit function

  * Costs or benefits associated with more probable values `\(x\)` get more weight

---
# An important general case involving integrals

 * Bayes' theorem (simplest form):

  * `\(P(A | B) = \frac{P(B | A) P(A)}{P(B)} = \frac{P(B | A) P(A)}{\int_{-\infty}^{\infty} P(B | x) P(x) dx}\)`

  * Calculation of normalizing constant requires marginalizing

 * This is just the simplest case of Bayes' theorem; there can be any number of variables involved

---

 * The previous examples were all stated for one variable

 * Often enough, we'll need to integrate over multiple variables

  * e.g. `\(\int_{-\infty}^{\infty} \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} P(A, B = x, C = y, D, E = z) dx dy dz\)`

 * There will always be one dimension per unknown variable, and any number of unknowns

  * therefore any number of dimensions of the integral

---
# Back to motivation 1

 * Random numbers can help us compute integrals such as `\(\int_A P(x) dx\)`

 * General strategy is to construct a sequence of numbers such that an average over the sequence converges to the integral.

  * i.e. a sequence `\(x_k\)` such that `\(sum_k f(x_k) \rightarrow \int_A P(x) f(x) dx\)`

 * That's great, what about other ways to approximate integrals?

---
# Other ways to evaluate integrals

 * Symbolic computation: work out integrals using algebra

  * Limited applicability, but great payoff if you can make it work

 * Quadrature rules: typically approximate function by polynomial and integrate that

  * Applicable to one or a few dimensions, but much more efficient than Monte Carlo methods

 * Low-discrepancy sequences, a.k.a. quasirandom numbers: more regular than random numbers, but less so than simple grid

  * Generally applicable, more efficient than Monte Carlo in maybe several dimensions

---

 * Monte Carlo methods are the most widely applicable method

 * Not the best method for every problem, but more or less the least-bad method if one size must fit all

---
# From a random sequence to an integral approximation

 * As we were saying, we need  `\(x_k\)` such that `\(sum_k f(x_k) \rightarrow \int_A P(x) f(x) dx\)`

 * It's convenient to work with sequences where `\(x_k\)` depends on `\(x_{k-1}\)`

 * These are examples of Markov chains

 * E.g. given `\(x_{k-1}\)`, take a jump in a random direction to get `\(x_k\)`

---
# Properties of Markov chains

 * Goal is to construct a Markov chain which has `\(P(x)\)` as its stationary distribution

  * i.e. if a cloud of points is distributed according to `\(P(x)\)`, then the distribution stays the same even as all the points jump from place to place

 * Whether the Markov chain has a unique stationary distribution depends on properties:

  * Detailed balance: just as likely to jump from point `\(a\)` to `\(b\)` as from `\(b\)` to `\(a\)`

  * Aperiodic, positive recurrent: might stay in same place, and will eventually come back if we do wander away

---

 * These are weak constraints; many different Markov chains have these properties

 * Varieties of Metropolis-Hastings and Gibbs samplers are widely used

 * "Hybrid Monte Carlo" makes an analogy to physics

 * "No U-Turn sampler" (NUTS) applies U-turn avoidance heuristic to hybrid Monte Carlo

---
# Back to solving problems


---
# Law of large numbers

    </textarea>
    <script src="https://remarkjs.com/downloads/remark-latest.min.js" type="text/javascript"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_HTML&delayStartupUntil=configured" type="text/javascript"></script>
    <script type="text/javascript">
      var slideshow = remark.create();

      // Setup MathJax
      MathJax.Hub.Config({
          tex2jax: {
          inlineMath: [['$','$'], ['\\(','\\)']],
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
          }
      });
      MathJax.Hub.Queue(function() {
          $(MathJax.Hub.getAllJax()).map(function(index, elem) {
              return(elem.SourceElement());
          }).parent().addClass('has-jax');
      });

      MathJax.Hub.Configured();
    </script>
  </body>
</html>
